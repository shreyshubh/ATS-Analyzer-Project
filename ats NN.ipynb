{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378eb91-c789-49a7-8152-f8dab1782708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Preparing Data ---\n",
      "--- Building New Neural Network Model ---\n",
      "--- Training the Classifier with Early Stopping ---\n",
      "Epoch 1/50\n",
      "25/25 - 11s - 437ms/step - accuracy: 0.1235 - loss: 3.1410 - val_accuracy: 0.0933 - val_loss: 3.2222\n",
      "Epoch 2/50\n",
      "25/25 - 6s - 247ms/step - accuracy: 0.2536 - loss: 2.9493 - val_accuracy: 0.1658 - val_loss: 2.8362\n",
      "Epoch 3/50\n",
      "25/25 - 6s - 240ms/step - accuracy: 0.3121 - loss: 2.3910 - val_accuracy: 0.4404 - val_loss: 1.9923\n",
      "Epoch 4/50\n",
      "25/25 - 7s - 266ms/step - accuracy: 0.5254 - loss: 1.6711 - val_accuracy: 0.6839 - val_loss: 1.2770\n",
      "Epoch 5/50\n",
      "25/25 - 6s - 240ms/step - accuracy: 0.7269 - loss: 1.0665 - val_accuracy: 0.7772 - val_loss: 0.8418\n",
      "Epoch 6/50\n",
      "25/25 - 6s - 241ms/step - accuracy: 0.8453 - loss: 0.6026 - val_accuracy: 0.8756 - val_loss: 0.5210\n",
      "Epoch 7/50\n",
      "25/25 - 6s - 252ms/step - accuracy: 0.9194 - loss: 0.3646 - val_accuracy: 0.9016 - val_loss: 0.3423\n",
      "Epoch 8/50\n",
      "25/25 - 6s - 252ms/step - accuracy: 0.9467 - loss: 0.2337 - val_accuracy: 0.9430 - val_loss: 0.2369\n",
      "Epoch 9/50\n",
      "25/25 - 6s - 252ms/step - accuracy: 0.9441 - loss: 0.2380 - val_accuracy: 0.9534 - val_loss: 0.1874\n",
      "Epoch 10/50\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "#                             ATS RESUME ANALYZER\n",
    "# =============================================================================\n",
    "# This script trains a neural network to classify resumes into job categories.\n",
    "# It then uses this model to analyze a user's resume, provide a compatibility\n",
    "# score for a chosen job, and offer feedback on missing keywords.\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# --- Step 1: Import Necessary Libraries ---\n",
    "# -----------------------------------------------------------------------------\n",
    "# We import all the tools we'll need for data handling, text processing,\n",
    "# building the neural network, and interacting with the user.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os # To check if the model file exists\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "\n",
    "# --- Step 2: Data Loading and Preparation ---\n",
    "# -----------------------------------------------------------------------------\n",
    "# Here, we load the datasets of resumes and job descriptions. The core task is\n",
    "# to prepare the data for the neural network by cleaning the text and\n",
    "# converting the job categories (labels) from text to numbers.\n",
    "\n",
    "print(\"--- Loading and Preparing Data ---\")\n",
    "# Load resumes (from 'job_descriptions.csv') and job descriptions\n",
    "resume_data = pd.read_csv('job_descriptions.csv')\n",
    "resume_data = resume_data[['Category', 'Resume']].dropna()\n",
    "jobs_data = pd.read_csv('UpdatedResumeDataSet.csv')\n",
    "jobs_data = jobs_data[['jobtitle', 'jobdescription']].dropna().rename(columns={'jobtitle': 'Category'})\n",
    "\n",
    "# Convert text labels (e.g., \"Data Science\") into numerical labels (e.g., 6)\n",
    "label_encoder = LabelEncoder()\n",
    "resume_data['category_encoded'] = label_encoder.fit_transform(resume_data['Category'])\n",
    "\n",
    "# Define our features (X) and labels (y)\n",
    "X = resume_data['Resume']\n",
    "y = resume_data['category_encoded']\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Define a function to clean the raw text\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # Remove special characters\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)   # Remove single-letter words\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function and tokenize the text (convert words to numbers)\n",
    "X_cleaned = X.apply(clean_text)\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(X_cleaned)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Pad all sequences to be the same length for the neural network\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "X_padded = pad_sequences(tokenizer.texts_to_sequences(X_cleaned), maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "\n",
    "# --- Step 3: Build or Load the Neural Network Model ---\n",
    "# -----------------------------------------------------------------------------\n",
    "# This is the core of the AI. We check if a pre-trained model exists. If not,\n",
    "# we define the architecture of our neural network (layers, etc.) and compile it.\n",
    "# This architecture is designed to understand sequences of text.\n",
    "\n",
    "MODEL_FILE_PATH = 'ats_classifier_model.keras'\n",
    "\n",
    "if os.path.exists(MODEL_FILE_PATH):\n",
    "    print(\"--- Loading Pre-trained Model ---\")\n",
    "    model = load_model(MODEL_FILE_PATH)\n",
    "else:\n",
    "    print(\"--- Building New Neural Network Model ---\")\n",
    "    EMBEDDING_DIM = 128\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM),\n",
    "        Bidirectional(LSTM(128, return_sequences=False)),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax') # Softmax for multi-class classification\n",
    "    ])\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # --- Step 4: Train the Model (only if not loaded) ---\n",
    "    # -------------------------------------------------------------------------\n",
    "    # If we built a new model, we train it on our data. We use EarlyStopping\n",
    "    # to prevent overfitting, automatically stopping when performance peaks.\n",
    "    # After training, we save the model for future use.\n",
    "\n",
    "    print(\"--- Training the Classifier with Early Stopping ---\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, callbacks=[early_stopping], verbose=2)\n",
    "    \n",
    "    # Save the newly trained model\n",
    "    model.save(MODEL_FILE_PATH)\n",
    "    print(f\"--- Model Trained and Saved as {MODEL_FILE_PATH} ---\")\n",
    "\n",
    "\n",
    "# --- Step 5: The Interactive Prediction Workflow ---\n",
    "# -----------------------------------------------------------------------------\n",
    "# This is the user-facing part of the application. It contains helper functions\n",
    "# for analyzing the resume and then runs a loop to get input from the user.\n",
    "\n",
    "def get_contextual_keywords(job_category, jobs_df):\n",
    "    \"\"\"Extracts keywords that are highly specific to a given job category.\"\"\"\n",
    "    target_docs = jobs_df[jobs_df['Category'] == job_category]['jobdescription'].apply(clean_text)\n",
    "    background_docs = jobs_df[jobs_df['Category'] != job_category]['jobdescription'].apply(clean_text)\n",
    "    if target_docs.empty: return []\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=2000, stop_words='english', ngram_range=(1,2))\n",
    "    tfidf_target = vectorizer.fit_transform(target_docs)\n",
    "    target_scores = np.array(tfidf_target.mean(axis=0)).ravel()\n",
    "\n",
    "    vectorizer_bg = TfidfVectorizer(vocabulary=vectorizer.vocabulary_, stop_words='english')\n",
    "    tfidf_bg = vectorizer_bg.fit_transform(background_docs)\n",
    "    bg_scores = np.array(tfidf_bg.mean(axis=0)).ravel()\n",
    "\n",
    "    specificity_scores = target_scores / (bg_scores + 1e-6)\n",
    "    sorted_indices = specificity_scores.argsort()[::-1]\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    return [feature_names[i] for i in sorted_indices[:15]]\n",
    "\n",
    "def get_missing_keywords(resume_text, job_category, jobs_df):\n",
    "    \"\"\"Finds important, context-aware keywords missing from the resume.\"\"\"\n",
    "    job_keywords = get_contextual_keywords(job_category, jobs_df)\n",
    "    cleaned_resume = clean_text(resume_text)\n",
    "    return [word for word in job_keywords if word not in cleaned_resume]\n",
    "\n",
    "def get_harsh_ats_score(original_score_percent):\n",
    "    \"\"\"Converts the model's confidence into a stricter, tiered score.\"\"\"\n",
    "    score = original_score_percent\n",
    "    if score >= 90: return np.random.randint(92, 98), \"Excellent Fit\"\n",
    "    elif score >= 75: return np.random.randint(85, 91), \"Strong Candidate\"\n",
    "    elif score >= 50: return np.random.randint(70, 82), \"Needs Improvement\"\n",
    "    elif score >= 25: return np.random.randint(55, 68), \"Significant Gaps\"\n",
    "    else: return np.random.randint(40, 52), \"Not a Match\"\n",
    "\n",
    "def read_pdf_text(file_path):\n",
    "    \"\"\"Extracts text content from a user-provided PDF file.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        return \"\".join(page.extract_text() for page in reader.pages)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nError: The file '{file_path}' was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred reading the PDF: {e}\")\n",
    "        return None\n",
    "        \n",
    "# --- Main Interactive Loop ---\n",
    "# This loop runs the program, asking the user for input and providing the final report.\n",
    "\n",
    "available_jobs = list(label_encoder.classes_)\n",
    "print(\"\\n--- ATS Resume Analyzer ---\")\n",
    "print(\"Available Job Categories:\")\n",
    "for job in available_jobs: print(f\"  {job}\")\n",
    "\n",
    "while True:\n",
    "    print(\"\\nPlease enter the job category you are interested in:\")\n",
    "    user_choice = input(\"> \")\n",
    "    if user_choice in available_jobs: break\n",
    "    else: print(f\"\\nError: '{user_choice}' is not a valid category.\")\n",
    "\n",
    "while True:\n",
    "    print(\"\\nPlease enter the name of your resume PDF file (e.g., my_resume.pdf):\")\n",
    "    my_resume_path = input(\"> \")\n",
    "    my_resume_text = read_pdf_text(my_resume_path)\n",
    "    if my_resume_text: break\n",
    "\n",
    "# --- Generate and Display the Final Report ---\n",
    "# 1. Get model's raw prediction\n",
    "cleaned_resume_text = clean_text(my_resume_text)\n",
    "sequence = tokenizer.texts_to_sequences([cleaned_resume_text])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "predictions = model.predict(padded_sequence, verbose=0)[0]\n",
    "chosen_category_index = list(label_encoder.classes_).index(user_choice)\n",
    "compatibility_score = predictions[chosen_category_index] * 100\n",
    "\n",
    "# 2. Get the harsh score and keyword feedback\n",
    "harsh_score, rating = get_harsh_ats_score(compatibility_score)\n",
    "missing = get_missing_keywords(my_resume_text, user_choice, jobs_data)\n",
    "\n",
    "# 3. Display the full, formatted report\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"      ATS ANALYSIS REPORT (Harsh)\")\n",
    "print(\"=\"*40)\n",
    "print(f\"JOB TARGETED: {user_choice}\")\n",
    "print(f\"FINAL ATS SCORE: {harsh_score} / 100\")\n",
    "print(f\"RATING: {rating}\")\n",
    "print(\"----------------------------------------\")\n",
    "if missing:\n",
    "    print(\"ANALYSIS: Resume lacks keywords specific to this role.\")\n",
    "    print(\"Consider highlighting skills and technologies like:\")\n",
    "    for keyword in missing:\n",
    "        print(f\"  - {keyword.title()}\")\n",
    "else:\n",
    "    print(\"ANALYSIS: Strong keyword alignment with the target role.\")\n",
    "print(\"----------------------------------------\")\n",
    "print(f\"(Note: Model's raw compatibility confidence was {compatibility_score:.1f}%)\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de0213e-2d15-473b-8fb3-0648106571fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
